{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Required:\n",
    "This Project uses only Keras for all purposes including a pre-trained model and Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Input\n",
    "import keras.backend as K\n",
    "from matplotlib.pyplot import imshow, show, imsave\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_image = \"style/image/path\"          #input your style image path\n",
    "content_image = \"content/image/path\"      #input your content image path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:\n",
    "Functions for Preprocesssing input, Losses and Regenerating the Stylized Image from Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    \"\"\"Function for loading and preprocessing input images.\n",
    "    This function uses VGG16's inbuilt preprocessing function.\n",
    "    \n",
    "    #Arguments\n",
    "        image_path: Path for the image.\n",
    "        \n",
    "    #Returns\n",
    "        Processed Image array with zero mean.\n",
    "    \"\"\"\n",
    "    img = image.load_img(image_path, target_size=(600,800))\n",
    "    img = image.img_to_array(img)\n",
    "    img = preprocess_input(img)           #preprocess_input function from vgg\n",
    "    img = np.expand_dims(img, axis=0)     #CNN input requires 4-D arrays of \n",
    "                                          #shape (batch_size, height, width, channels)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    \"\"\"Function for regenerating stylized image from Model Output\n",
    "    Since the Images were zero-centered and were converted to \n",
    "    float, we need a deprocessing function to retrieve a displayable\n",
    "    image. \n",
    "    \n",
    "    #Arguments\n",
    "        x: A 4-D array containing a single processed image.\n",
    "        \n",
    "    #Returns\n",
    "        x: A 3-D array containing the output image.\n",
    "    \"\"\"\n",
    "    x = x[0]\n",
    "    x[:, :, 0] += 103.939                 #These are the approximate means,\n",
    "    x[:, :, 1] += 116.779                 #precomuted channel-wise.\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')#integer type for image array\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(A):\n",
    "    \"\"\"Function for computing correlation between different output\n",
    "       channels.\n",
    "    \n",
    "    #Arguments\n",
    "        A: A 4-D tensor output of a layer in the model.\n",
    "        \n",
    "    #Returns\n",
    "        Gram-matrix of A.\n",
    "    \"\"\"\n",
    "    return K.dot(A, K.transpose(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_cost():\n",
    "    \"\"\"Function for computing the dissimilarity between Generated\n",
    "       image and input content image.\n",
    "    This uses normalised Mean Square Error (MSE).\n",
    "    \n",
    "    #Arguments\n",
    "        None\n",
    "        \n",
    "    #Returns\n",
    "        Computed Content Cost in a tensor.\n",
    "    \"\"\"\n",
    "    J_content = K.variable(0.)\n",
    "    \n",
    "    a_G = model.layers[12].output\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    a_G_unrolled = K.reshape(a_G,(-1, n_H*n_W, n_C))\n",
    "    \n",
    "    J_content = (1/(4*n_H*n_W*n_C))*K.sum(K.square(a_C_unrolled-a_G_unrolled))\n",
    "    return J_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_layer_style_cost(a_S, a_G):\n",
    "    \"\"\"Function for computing dissimilarity between Style Image\n",
    "       output and Generated Image at a given layer.\n",
    "    \n",
    "    #Arguments\n",
    "        a_S: Style Image Activations\n",
    "        a_G: Generated Image Activation\n",
    "    \n",
    "    #Returns\n",
    "        J_style_layer: Computed cost at a layer in a tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    J_style_layer = K.variable(0.)\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "   \n",
    "    a_S = K.transpose(K.reshape(a_S, (n_H*n_W,n_C)))\n",
    "    a_G = K.transpose(K.reshape(a_G, (n_H*n_W,n_C)))\n",
    "\n",
    "    GS = gram_matrix(a_S)\n",
    "    GG = gram_matrix(a_G)\n",
    "\n",
    "\n",
    "    J_style_layer = K.square(1/(2*n_C*n_H*n_W))*K.sum(K.sum(K.square(GS-GG)))\n",
    "    \n",
    "    return J_style_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_style_cost(model=model, STYLE_LAYERS=style_layers):\n",
    "    \"\"\"Function for computing Average Style Cost across the layers.\n",
    "    \n",
    "    #Arguments\n",
    "        model: Keras Model Instance\n",
    "        STYLE_LAYERS: List of Style Layer indices to be included in \n",
    "                      calculation.\n",
    "    \n",
    "    #Returns\n",
    "        J_style: Computed Style Cost across layers in a variable tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_style = K.variable(0.)\n",
    "    i=0\n",
    "\n",
    "    for layer in STYLE_LAYERS:\n",
    "        \n",
    "        a_G = model.layers[layer].output\n",
    "\n",
    "        J_style_layer = compute_layer_style_cost(style_features[i], a_G)\n",
    "\n",
    "        J_style += 0.25 * J_style_layer\n",
    "        i+=1\n",
    "\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_cost(alpha = 1000, beta = 4):\n",
    "    \"\"\"Function for computing Final Cost as a weighted average of \n",
    "       Style and Content Costs.\n",
    "    \n",
    "    #Arguments\n",
    "        alpha: Weight of content cost\n",
    "        beta: weight of style cost\n",
    "    \n",
    "    #Returns\n",
    "        J: weighted sum of style and content cost\n",
    "    \"\"\"\n",
    "\n",
    "    J = K.variable(0.)\n",
    "    J = (alpha*content_cost()+beta*compute_style_cost())\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_image = preprocess(style_image)\n",
    "content_image = preprocess(content_image)\n",
    "gen_image = K.variable(np.random.randn(1,600,800,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"style_image \",style_image.shape)\n",
    "print(\"content_image \",content_image.shape)\n",
    "print(\"gen_image \",gen_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_layers = [1,4,7,11]               #Inclusion of more layers produces\n",
    "                                        #better results at the cost of \n",
    "                                        #computational complexity\n",
    "                                        #try [1,4,7,11,15]\n",
    "content_layers = [12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Keras VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG16(include_top=False, weights='imagenet', input_tensor=Input(tensor=gen_image))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Style and Content Activations for Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "style_features = []\n",
    "for layer in style_layers:\n",
    "    fn = K.function([model.layers[0].input], [model.layers[layer].output])\n",
    "    style_features.append(fn([style_image])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_features = []\n",
    "for layer in content_layers:\n",
    "    fn = K.function([model.layers[0].input], [model.layers[layer].output])\n",
    "    content_features.append(fn([content_image])[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in style_features:\n",
    "    print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, w, h, c = a_C.shape()\n",
    "a_C_unrolled = K.reshape(content_features[0], (-1,w*h,c))\n",
    "del content_features\n",
    "print(\"a_C_unrolled \", a_C_unrolled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_loss = total_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=2.0)\n",
    "updates = opt.get_updates([gen_image],[],total_loss)\n",
    "train = K.function([],[total_loss],updates)\n",
    "print(\"Training Step defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    e_start = time()\n",
    "    out = train([])\n",
    "    e_end = time()\n",
    "    print(\"Epoch: {}, Loss: {:.2e}, Time taken per Step: {:.2f}\".format(epoch, out[0], e_end-e_start))\n",
    "    if(epoch%20==0):\n",
    "        imshow(deprocess_image(K.get_value(gen_image)))\n",
    "        show()\n",
    "        print(\"ETA: {}\".format((e_end-start)*(500-epoch)/(epoch+1)))\n",
    "print(\"Total time taken: {:.2f}\".format(time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_out = deprocess_image(K.get_value(gen_image))\n",
    "imsave(final_out,\"styled_image.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
